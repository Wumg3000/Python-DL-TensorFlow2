{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6030b265",
   "metadata": {},
   "source": [
    "## <font color=blue>本脚本为参考资料</font>\n",
    "用于连续动作的PPO\n",
    "游戏规则：杆子进行转动，通过训练，使杆子能够朝上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e821c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于连续动作的PPO\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b362d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- #\n",
    "# 策略网络--输出连续动作的高斯分布的均值和标准差\n",
    "# ------------------------------------- #\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, n_states, n_hiddens, n_actions):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, n_hiddens)\n",
    "        self.fc_mu = nn.Linear(n_hiddens, n_actions)\n",
    "        self.fc_std = nn.Linear(n_hiddens, n_actions)\n",
    "    # 前向传播\n",
    "    def forward(self, x):  # \n",
    "        x = self.fc1(x)  # [b, n_states] --> [b, n_hiddens]\n",
    "        x = F.relu(x)\n",
    "        mu = self.fc_mu(x)  # [b, n_hiddens] --> [b, n_actions]\n",
    "        mu = 2 * torch.tanh(mu)  # 值域 [-2,2]\n",
    "        std = self.fc_std(x)  # [b, n_hiddens] --> [b, n_actions]\n",
    "        std = F.softplus(std)  # 值域 小于0的部分逼近0，大于0的部分几乎不变\n",
    "        return mu, std\n",
    "\n",
    "# ------------------------------------- #\n",
    "# 价值网络 -- 评估当前状态的价值\n",
    "# ------------------------------------- #\n",
    "\n",
    "class ValueNet(nn.Module):\n",
    "    def __init__(self, n_states, n_hiddens):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_states, n_hiddens)\n",
    "        self.fc2 = nn.Linear(n_hiddens, 1)\n",
    "    # 前向传播\n",
    "    def forward(self, x):  \n",
    "        x = self.fc1(x)  # [b,n_states]-->[b,n_hiddens]\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)  # [b,n_hiddens]-->[b,1]\n",
    "        return x\n",
    "\n",
    "# ------------------------------------- #\n",
    "# 模型构建--处理连续动作\n",
    "# ------------------------------------- #\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, n_states, n_hiddens, n_actions,\n",
    "                 actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, gamma, device):\n",
    "        # 实例化策略网络\n",
    "        self.actor = PolicyNet(n_states, n_hiddens, n_actions).to(device)\n",
    "        # 实例化价值网络\n",
    "        self.critic = ValueNet(n_states, n_hiddens).to(device)\n",
    "        # 策略网络的优化器\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        # 价值网络的优化器\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # 属性分配\n",
    "        self.lmbda = lmbda  # GAE优势函数的缩放因子\n",
    "        self.epochs = epochs  # 一条序列的数据用来训练多少轮\n",
    "        self.eps = eps  # 截断范围\n",
    "        self.gamma = gamma  # 折扣系数\n",
    "        self.device = device \n",
    "    \n",
    "    # 动作选择\n",
    "    def take_action(self, state):  # 输入当前时刻的状态\n",
    "        # [n_states]-->[1,n_states]-->tensor\n",
    "        state = torch.tensor(state[np.newaxis, :]).to(self.device)\n",
    "        # 预测当前状态的动作，输出动作概率的高斯分布\n",
    "        mu, std = self.actor(state)\n",
    "        # 构造高斯分布\n",
    "        action_dict = torch.distributions.Normal(mu, std)\n",
    "        # 随机选择动作\n",
    "        action = action_dict.sample().item()\n",
    "        return [action]  # 返回动作值\n",
    "\n",
    "    # 训练\n",
    "    def update(self, transition_dict):\n",
    "        # 提取数据集\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)  # [b,n_states]\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.float).view(-1,1).to(self.device)  # [b,1]\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1,1).to(self.device)  # [b,1]\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)  # [b,n_states]\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1,1).to(self.device)  # [b,1]\n",
    "        \n",
    "        # 价值网络--目标，获取下一时刻的state_value  [b,n_states]-->[b,1]\n",
    "        next_states_target = self.critic(next_states)\n",
    "        # 价值网络--目标，当前时刻的state_value  [b,1]\n",
    "        td_target = rewards + self.gamma * next_states_target * (1-dones)\n",
    "        # 价值网络--预测，当前时刻的state_value  [b,n_states]-->[b,1]\n",
    "        td_value = self.critic(states)\n",
    "        # 时序差分，预测值-目标值  # [b,1]\n",
    "        td_delta = td_value - td_target\n",
    "\n",
    "        # 对时序差分结果计算GAE优势函数\n",
    "        td_delta = td_delta.cpu().detach().numpy()  # [b,1]\n",
    "        advantage_list = []  # 保存每个时刻的优势函数\n",
    "        advantage = 0  # 优势函数初始值\n",
    "        # 逆序遍历时序差分结果，把最后一时刻的放前面\n",
    "        for delta in td_delta[::-1]:\n",
    "            advantage = self.gamma * self.lmbda * advantage + delta\n",
    "            advantage_list.append(advantage)\n",
    "        # 正序排列优势函数\n",
    "        advantage_list.reverse()\n",
    "        # numpy --> tensor\n",
    "        advantage = torch.tensor(advantage_list, dtype=torch.float).to(self.device)\n",
    "\n",
    "        # 策略网络--预测，当前状态选择的动作的高斯分布\n",
    "        mu, std = self.actor(states)  # [b,1]\n",
    "        # 基于均值和标准差构造正态分布\n",
    "        action_dists = torch.distributions.Normal(mu.detch(), std.detch())\n",
    "        # 从正态分布中选择动作，并使用log函数\n",
    "        old_log_prob = action_dists.log_prob(actions)\n",
    "\n",
    "        # 一个序列训练epochs次\n",
    "        for _ in range(self.epochs):\n",
    "            # 预测当前状态下的动作\n",
    "            mu, std = self.actor(states)\n",
    "            # 构造正态分布\n",
    "            action_dists = torch.distributions.Normal(mu, std)\n",
    "            # 当前策略在 t 时刻智能体处于状态 s 所采取的行为概率\n",
    "            log_prob = action_dists.log_prob(actions)\n",
    "            # 计算概率的比值来控制新策略更新幅度\n",
    "            ratio = torch.exp(log_prob - old_log_prob)\n",
    "            \n",
    "            # 公式的左侧项\n",
    "            surr1 = ratio * advantage\n",
    "            # 公式的右侧项，截断\n",
    "            surr2 = torch.clamp(ratio, 1-self.eps, 1+self.eps)\n",
    "\n",
    "            # 策略网络的损失PPO-clip\n",
    "            actor_loss = torch.mean(-torch.min(surr1,surr2))\n",
    "            # 价值网络的当前时刻预测值，与目标价值网络当前时刻的state_value之差\n",
    "            critic_loss = torch.mean(F.mse_loss(self.critic(states), td_target.detch()))\n",
    "\n",
    "            # 优化器清0\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            # 梯度反传\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            # 参数更新\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd9125cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5108\\2428639825.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 动作选择\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 环境更新\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[1;31m# 保存每个时刻的状态\\动作\\...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[0mtransition_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'states'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\wrappers\\env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, u)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mcosts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\pendulum.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender_mode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"human\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtick\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"render_fps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m             \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 参数设置\n",
    "# ----------------------------------------- #\n",
    "\n",
    "num_episodes = 1  # 总迭代次数\n",
    "gamma = 0.9  # 折扣因子\n",
    "actor_lr = 1e-3  # 策略网络的学习率\n",
    "critic_lr = 1e-2  # 价值网络的学习率\n",
    "n_hiddens = 16  # 隐含层神经元个数\n",
    "env_name = 'Pendulum-v1'  # 连续环境\n",
    "return_list = []  # 保存每个回合的return\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 环境加载\n",
    "# ----------------------------------------- #\n",
    "\n",
    "env = gym.make(env_name, render_mode=\"human\")\n",
    "n_states = env.observation_space.shape[0]  # 状态数 3\n",
    "n_actions = env.action_space.shape[0]  # 动作数 1\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 模型构建\n",
    "# ----------------------------------------- #\n",
    "\n",
    "agent = PPO(n_states=n_states,  # 状态数3\n",
    "            n_hiddens=n_hiddens,  # 隐含层数\n",
    "            n_actions=n_actions,  # 动作数1\n",
    "            actor_lr=actor_lr,  # 策略网络学习率\n",
    "            critic_lr=critic_lr,  # 价值网络学习率\n",
    "            lmbda = 0.95,  # 优势函数的缩放因子\n",
    "            epochs = 10,  # 一组序列训练的轮次\n",
    "            eps = 0.2,  # PPO中截断范围的参数\n",
    "            gamma=gamma,  # 折扣因子\n",
    "            device = device\n",
    "            )\n",
    "\n",
    "# ----------------------------------------- #\n",
    "# 训练--回合更新 on_policy\n",
    "# ----------------------------------------- #\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    state = env.reset()[0]  # 环境重置\n",
    "    done = False  # 任务完成的标记\n",
    "    episode_return = 0  # 累计每回合的reward\n",
    "\n",
    "    # 构造数据集，保存每个回合的状态数据\n",
    "    transition_dict = {\n",
    "        'states': [],\n",
    "        'actions': [],\n",
    "        'next_states': [],\n",
    "        'rewards': [],\n",
    "        'dones': [],\n",
    "    }\n",
    "\n",
    "    while not done:\n",
    "        action = agent.take_action(state)  # 动作选择\n",
    "        next_state, reward, done, _, _  = env.step(action)  # 环境更新\n",
    "        # 保存每个时刻的状态\\动作\\...\n",
    "        transition_dict['states'].append(state)\n",
    "        transition_dict['actions'].append(action)\n",
    "        transition_dict['next_states'].append(next_state)\n",
    "        transition_dict['rewards'].append(reward)\n",
    "        transition_dict['dones'].append(done)\n",
    "        # 更新状态\n",
    "        state = next_state\n",
    "        # 累计回合奖励\n",
    "        episode_return += reward\n",
    "\n",
    "    # 保存每个回合的return\n",
    "    return_list.append(episode_return)\n",
    "    # 模型训练\n",
    "    agent.learn(transition_dict)\n",
    "\n",
    "    # 打印回合信息\n",
    "    print(f'iter:{i}, return:{np.mean(return_list[-10:])}')\n",
    "    \n",
    "print('循环完成')\n",
    "env.render() # 图像引擎\n",
    "env.close() # 关闭环境\n",
    "\n",
    "# -------------------------------------- #\n",
    "# 绘图\n",
    "# -------------------------------------- #\n",
    "\n",
    "plt.plot(return_list)\n",
    "plt.title('return')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cc6b54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
