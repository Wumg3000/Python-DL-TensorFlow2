{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20.2 Q-Learning算法实例\n",
    "\t本节使用Q-Learning算法实现悬崖徒步，具体实现方式与SARSA基本一致。由于Q-Learning采用贪心选择策略，即通过np.argmax(Q(x,y,;))来获取动作a。此外，选择开拓还是探索动作在while循环内部，因为它是在探索时贪心选择动作。主要代码如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "nrows = 4\n",
    "ncols = 12\n",
    "nact = 4\n",
    "\n",
    "nepisodes = 100000\n",
    "epsilon = 0.1\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "\n",
    "reward_normal = -1\n",
    "reward_cliff = -100\n",
    "reward_destination = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((nrows,ncols,nact),dtype=np.float)\n",
    "\n",
    "def go_to_start():\n",
    "  # start coordinates \n",
    "  y = nrows\n",
    "  x = 0\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def random_action():\n",
    "  # a = 0 : top/north\n",
    "  # a = 1 : right/east\n",
    "  # a = 2 : bottom/south\n",
    "  # a = 3 : left/west\n",
    "  a = np.random.randint(nact)\n",
    "  return a\n",
    "\n",
    "\n",
    "def move(x,y,a):\n",
    "  # state = 0: OK\n",
    "  # state = 1: reached destination\n",
    "  # state = 2: fell into cliff\n",
    "  state = 0 \n",
    "\n",
    "  if (x == 0 and y == nrows and a == 0):\n",
    "    # start location\n",
    "    x1 = x\n",
    "    y1 = y - 1 \n",
    "    return x1, y1, state  \n",
    "  elif (x == ncols-1 and y == nrows-1 and a == 2):\n",
    "    # reached destination\n",
    "    x1 = x\n",
    "    y1 = y + 1\n",
    "    state = 1\n",
    "    return x1, y1, state\n",
    "  else: \n",
    "    if (a == 0):\n",
    "      x1 = x\n",
    "      y1 = y - 1\n",
    "    elif (a == 1):\n",
    "      x1 = x + 1\n",
    "      y1 = y\n",
    "    elif (a == 2):\n",
    "      x1 = x\n",
    "      y1 = y + 1\n",
    "    elif (a == 3):\n",
    "      x1 = x - 1 \n",
    "      y1 = y\n",
    "    if (x1 < 0):\n",
    "     x1 = 0\n",
    "    if (x1 > ncols-1):\n",
    "     x1 = ncols-1\n",
    "    if (y1 < 0):\n",
    "     y1 = 0\n",
    "    if (y1 > nrows-1):\n",
    "     state = 2\n",
    "    return x1, y1, state    \n",
    "    \n",
    "\n",
    "\n",
    "def exploit(x,y,Q):\n",
    "   # start location\n",
    "   if (x == 0 and y == nrows):\n",
    "     a = 0\n",
    "     return a \n",
    "   # destination location\n",
    "   if (x == ncols-1 and y == nrows-1):\n",
    "     a = 2\n",
    "     return a\n",
    "   if (x == ncols-1 and y == nrows):\n",
    "     print(\"exploit at destination not possible \")\n",
    "     sys.exit()\n",
    "   # interior location\n",
    "   if (x < 0 or x > ncols-1 or y < 0 or y > nrows-1):\n",
    "     print(\"error \", x, y)\n",
    "     sys.exit()\n",
    "   a = np.argmax(Q[y,x,:]) \n",
    "   return a\n",
    "\n",
    "\n",
    "def bellman(x,y,a,reward,Qs1a1,Q):\n",
    "   if (y == nrows and x == 0):\n",
    "     # at start location; no Bellman update possible\n",
    "     return Q\n",
    "   if (y == nrows and x == ncols-1):\n",
    "     # at destination location; no Bellman update possible\n",
    "     return Q\n",
    "   Q[y,x,a] = Q[y,x,a] + alpha*(reward + gamma*Qs1a1 - Q[y,x,a])\n",
    "   return Q\n",
    "\n",
    "\n",
    "def max_Q(x,y,Q):\n",
    "  a = np.argmax(Q[y,x,:]) \n",
    "  return Q[y,x,a]\n",
    "\n",
    "\n",
    "def explore_exploit(x,y,Q):\n",
    "  # if we end up at the start location, then exploit\n",
    "  if (x == 0 and y == nrows):\n",
    "    a = 0\n",
    "    return a\n",
    "\n",
    "  r = np.random.uniform()\n",
    "  if (r < epsilon):\n",
    "    # explore\n",
    "    a = random_action()\n",
    "  else:\n",
    "    # exploit\n",
    "    a = exploit(x,y,Q) \n",
    "  return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode #:  0\n",
      "episode #:  5000\n",
      "episode #:  10000\n",
      "episode #:  15000\n",
      "episode #:  20000\n",
      "episode #:  25000\n",
      "episode #:  30000\n",
      "episode #:  35000\n",
      "episode #:  40000\n",
      "episode #:  45000\n",
      "episode #:  50000\n",
      "episode #:  55000\n",
      "episode #:  60000\n",
      "episode #:  65000\n",
      "episode #:  70000\n",
      "episode #:  75000\n",
      "episode #:  80000\n",
      "episode #:  85000\n",
      "episode #:  90000\n",
      "episode #:  95000\n",
      "episode #:  100000\n"
     ]
    }
   ],
   "source": [
    "for n in range(nepisodes+1):\n",
    "  if (n % 5000 == 0): \n",
    "    print(\"episode #: \", n)\n",
    "  x, y = go_to_start()\n",
    "\n",
    "  \n",
    "  while(True):\n",
    "   a = explore_exploit(x,y,Q)\n",
    "   x1, y1, state = move(x,y,a)\n",
    "   if (state == 1):\n",
    "     reward = reward_destination\n",
    "     Qs1a1 = 0.0\n",
    "     Q = bellman(x,y,a,reward,Qs1a1,Q)\n",
    "     break \n",
    "   elif (state == 2):         \n",
    "     reward = reward_cliff\n",
    "     Qs1a1 = 0.0\n",
    "     Q = bellman(x,y,a,reward,Qs1a1,Q)\n",
    "     break\n",
    "   elif (state == 0):\n",
    "    reward = reward_normal\n",
    "    # Sarsa，执行策略\n",
    "    #Qs1a1 = Q[y1,x1,a1]  #SARSA更新方法\n",
    "    Qs1a1 = max_Q(x1,y1,Q)  #Q-Learning更新方法\n",
    "    Q = bellman(x,y,a,reward,Qs1a1,Q)\n",
    "    x = x1\n",
    "    y = y1\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(nact):\n",
    " plt.subplot(nact,1,i+1)\n",
    " plt.imshow(Q[:,:,i])\n",
    " plt.axis('off')\n",
    " plt.colorbar()\n",
    " if (i == 0):\n",
    "   plt.title('Q-north')\n",
    " elif (i == 1):\n",
    "   plt.title('Q-east')\n",
    " elif (i == 2):\n",
    "   plt.title('Q-south')\n",
    " elif (i == 3):\n",
    "   plt.title('Q-west')    \n",
    "plt.savefig('Q_qlearning.png')\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x y a\n",
      "0 4 0\n",
      "0 3 1\n",
      "1 3 1\n",
      "2 3 1\n",
      "3 3 1\n",
      "4 3 1\n",
      "5 3 1\n",
      "6 3 1\n",
      "7 3 1\n",
      "8 3 1\n",
      "9 3 1\n",
      "10 3 1\n",
      "11 3 2\n",
      "breaking  1\n",
      "done\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# path planning\n",
    "print(\"x\",\"y\",\"a\")\n",
    "path = np.zeros((nrows,ncols,nact),dtype=np.float)\n",
    "\n",
    "x, y = go_to_start()\n",
    "while(True):\n",
    "   a = exploit(x,y,Q) \n",
    "   print(x,y,a)\n",
    "   x1, y1, state = move(x,y,a)\n",
    "   if (state == 1 or state == 2):\n",
    "     print(\"breaking \", state)\n",
    "     break \n",
    "   elif (state == 0):     \n",
    "     x = x1\n",
    "     y = y1\n",
    "     if (x >= 0 and x <= ncols-1 and y >= 0 and y <= nrows-1):\n",
    "       path[y,x] = 100.0\n",
    "\n",
    "path = np.array(path).astype(np.uint8)\n",
    "\n",
    "plt.imshow(path,cmap=plt.cm.jet)\n",
    "plt.savefig('path_sarsa.png')\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning 是一种贪心策略，因此，agent采用了靠近悬崖的路径，这是最短也最大胆的一条路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
