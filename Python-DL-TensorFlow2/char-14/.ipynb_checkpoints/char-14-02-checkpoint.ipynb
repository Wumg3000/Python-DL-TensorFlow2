{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from nets.frcnn import get_model\n",
    "from nets.frcnn_training import (ProposalTargetCreator, classifier_cls_loss,\n",
    "                                 classifier_smooth_l1, rpn_cls_loss,\n",
    "                                 rpn_smooth_l1)\n",
    "from utils.anchors import get_anchors\n",
    "from utils.callbacks import LossHistory\n",
    "from utils.dataloader import FRCNNDatasets\n",
    "from utils.utils import get_classes\n",
    "from utils.utils_bbox import BBoxUtility\n",
    "from utils.utils_fit import fit_one_epoch\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(device_type='GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型\n",
    "从model_data/voc_weights_resnet.h5获取预训练模型  \n",
    "每次训练好的模型存放在logs目录下  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights ../data/char-14/model_data/voc_weights_resnet.h5.\n",
      "Freeze the first 141 layers of total 185 layers.\n",
      "Train on 2501 samples, val on 2510 samples, with batch size 4.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:   0%|                                                                 | 0/625 [00:00<?, ?it/s<class 'dict'>]C:\\Users\\wumgapp\\Anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n",
      "Epoch 1/4: 100%|█| 625/625 [08:28<00:00,  1.23it/s, lr=1e-04, roi_cls=0.206, roi_loc=0.41, rpn_cls=0.133, rpn_loc=0.385, total=1.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|█████████████████████████████████████████████████████████| 627/627 [04:36<00:00,  2.27it/s, total=1.07]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/4\n",
      "Total Loss: 1.134 || Val Loss: 1.071 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|█| 625/625 [08:39<00:00,  1.20it/s, lr=9.6e-5, roi_cls=0.2, roi_loc=0.405, rpn_cls=0.135, rpn_loc=0.388, total=1.13]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|█████████████████████████████████████████████████████████| 627/627 [04:50<00:00,  2.16it/s, total=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:2/4\n",
      "Total Loss: 1.127 || Val Loss: 1.032 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|█| 625/625 [08:57<00:00,  1.16it/s, lr=9.22e-5, roi_cls=0.195, roi_loc=0.389, rpn_cls=0.136, rpn_loc=0.377, total=1.1]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|█████████████████████████████████████████████████████████| 627/627 [05:00<00:00,  2.09it/s, total=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:3/4\n",
      "Total Loss: 1.097 || Val Loss: 1.018 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|█| 625/625 [2:34:59<00:00, 14.88s/it, lr=8.85e-5, roi_cls=0.191, roi_loc=0.373, rpn_cls=0.14, rpn_loc=0.378, total=1.08]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|█████████████████████████████████████████████████████████| 627/627 [05:14<00:00,  1.99it/s, total=1.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:4/4\n",
      "Total Loss: 1.081 || Val Loss: 1.024 \n",
      "Train on 2501 samples, val on 2510 samples, with batch size 2.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "'''\n",
    "训练自己的目标检测模型一定需要注意以下几点：\n",
    "1、训练前仔细检查自己的格式是否满足要求，该库要求数据集格式为VOC格式，需要准备好的内容有输入图片和标签\n",
    "   输入图片为.jpg图片，无需固定大小，传入训练前会自动进行resize。\n",
    "   灰度图会自动转成RGB图片进行训练，无需自己修改。\n",
    "   输入图片如果后缀非jpg，需要自己批量转成jpg后再开始训练。\n",
    "\n",
    "   标签为.xml格式，文件中会有需要检测的目标信息，标签文件和输入图片文件相对应。\n",
    "\n",
    "2、训练好的权值文件保存在logs文件夹中，每个epoch都会保存一次，如果只是训练了几个step是不会保存的，epoch和step的概念要捋清楚一下。\n",
    "   在训练过程中，该代码并没有设定只保存最低损失的，因此按默认参数训练完会有100个权值，如果空间不够可以自行删除。\n",
    "   这个并不是保存越少越好也不是保存越多越好，有人想要都保存、有人想只保存一点，为了满足大多数的需求，还是都保存可选择性高。\n",
    "\n",
    "3、损失值的大小用于判断是否收敛，比较重要的是有收敛的趋势，即验证集损失不断下降，如果验证集损失基本上不改变的话，模型基本上就收敛了。\n",
    "   损失值的具体大小并没有什么意义，大和小只在于损失的计算方式，并不是接近于0才好。如果想要让损失好看点，可以直接到对应的损失函数里面除上10000。\n",
    "   训练过程中的损失值会保存在logs文件夹下的loss_%Y_%m_%d_%H_%M_%S文件夹中\n",
    "\n",
    "4、调参是一门蛮重要的学问，没有什么参数是一定好的，现有的参数是我测试过可以正常训练的参数，因此我会建议用现有的参数。\n",
    "   但是参数本身并不是绝对的，比如随着batch的增大学习率也可以增大，效果也会好一些；过深的网络不要用太大的学习率等等。\n",
    "   这些都是经验上，只能靠各位同学多查询资料和自己试试了。\n",
    "'''  \n",
    "#--------------------------------------------------------#\n",
    "#   训练前一定要修改classes_path，使其对应自己的数据集\n",
    "#--------------------------------------------------------#\n",
    "classes_path    = 'model_data/voc_classes.txt'\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "#   权值文件的下载请看README，可以通过网盘下载。模型的 预训练权重 对不同数据集是通用的，因为特征是通用的。\n",
    "#   模型的 预训练权重 比较重要的部分是 主干特征提取网络的权值部分，用于进行特征提取。\n",
    "#   预训练权重对于99%的情况都必须要用，不用的话主干部分的权值太过随机，特征提取效果不明显，网络训练的结果也不会好\n",
    "#\n",
    "#   如果训练过程中存在中断训练的操作，可以将model_path设置成logs文件夹下的权值文件，将已经训练了一部分的权值再次载入。\n",
    "#   同时修改下方的 冻结阶段 或者 解冻阶段 的参数，来保证模型epoch的连续性。\n",
    "#   \n",
    "#   当model_path = ''的时候不加载整个模型的权值。\n",
    "#\n",
    "#   此处使用的是整个模型的权重，因此是在train.py进行加载的。\n",
    "#   如果想要让模型从主干的预训练权值开始训练，则设置model_path为主干网络的权值，此时仅加载主干。\n",
    "#   如果想要让模型从0开始训练，则设置model_path = ''，Freeze_Train = Fasle，此时从0开始训练，且没有冻结主干的过程。\n",
    "#   一般来讲，从0开始训练效果会很差，因为权值太过随机，特征提取效果不明显。\n",
    "#\n",
    "#   网络一般不从0开始训练，至少会使用主干部分的权值，有些论文提到可以不用预训练，主要原因是他们 数据集较大 且 调参能力优秀。\n",
    "#   如果一定要训练网络的主干部分，可以了解imagenet数据集，首先训练分类模型，分类模型的 主干部分 和该模型通用，基于此进行训练。\n",
    "#----------------------------------------------------------------------------------------------------------------------------#\n",
    "model_path      = '../data/char-14/model_data/voc_weights_resnet.h5'\n",
    "#------------------------------------------------------#\n",
    "#   输入的shape大小\n",
    "#------------------------------------------------------#\n",
    "input_shape     = [600, 600]\n",
    "#---------------------------------------------#\n",
    "#   vgg或者resnet50\n",
    "#---------------------------------------------#\n",
    "backbone        = \"resnet50\"\n",
    "#------------------------------------------------------------------------#\n",
    "#   anchors_size用于设定先验框的大小，每个特征点均存在9个先验框。\n",
    "#   anchors_size每个数对应3个先验框。\n",
    "#   当anchors_size = [8, 16, 32]的时候，生成的先验框宽高约为：\n",
    "#   [90, 180] ; [180, 360]; [360, 720]; [128, 128]; \n",
    "#   [256, 256]; [512, 512]; [180, 90] ; [360, 180]; \n",
    "#   [720, 360]; 详情查看anchors.py\n",
    "#   如果想要检测小物体，可以减小anchors_size靠前的数。\n",
    "#   比如设置anchors_size = [64, 256, 512]\n",
    "#------------------------------------------------------------------------#\n",
    "anchors_size    = [128, 256, 512]\n",
    "\n",
    "#----------------------------------------------------#\n",
    "#   训练分为两个阶段，分别是冻结阶段和解冻阶段。\n",
    "#   显存不足与数据集大小无关，提示显存不足请调小batch_size。\n",
    "#----------------------------------------------------#\n",
    "#----------------------------------------------------#\n",
    "#   冻结阶段训练参数\n",
    "#   此时模型的主干被冻结了，特征提取网络不发生改变\n",
    "#   占用的显存较小，仅对网络进行微调\n",
    "#----------------------------------------------------#\n",
    "Init_Epoch          = 0\n",
    "Freeze_Epoch        = 4\n",
    "Freeze_batch_size   = 4\n",
    "Freeze_lr           = 1e-4\n",
    "#----------------------------------------------------#\n",
    "#   解冻阶段训练参数\n",
    "#   此时模型的主干不被冻结了，特征提取网络会发生改变\n",
    "#   占用的显存较大，网络所有的参数都会发生改变\n",
    "#----------------------------------------------------#\n",
    "UnFreeze_Epoch      = 4\n",
    "Unfreeze_batch_size = 2\n",
    "Unfreeze_lr         = 1e-5\n",
    "#------------------------------------------------------#\n",
    "#   是否进行冻结训练，默认先冻结主干训练后解冻训练。\n",
    "#------------------------------------------------------#\n",
    "Freeze_Train        = True\n",
    "#----------------------------------------------------#\n",
    "#   获得图片路径和标签\n",
    "#----------------------------------------------------#\n",
    "train_annotation_path   = '2007_train.txt'\n",
    "val_annotation_path     = '2007_val.txt'\n",
    "\n",
    "#----------------------------------------------------#\n",
    "#   获取classes和anchor\n",
    "#----------------------------------------------------#\n",
    "class_names, num_classes = get_classes(classes_path)\n",
    "num_classes += 1\n",
    "anchors = get_anchors(input_shape, backbone, anchors_size)\n",
    "\n",
    "K.clear_session()\n",
    "model_rpn, model_all = get_model(num_classes, backbone = backbone)\n",
    "if model_path != '':\n",
    "    #------------------------------------------------------#\n",
    "    #   载入预训练权重\n",
    "    #------------------------------------------------------#\n",
    "    print('Load weights {}.'.format(model_path))\n",
    "    model_rpn.load_weights(model_path, by_name=True)\n",
    "    model_all.load_weights(model_path, by_name=True)\n",
    "\n",
    "#--------------------------------------------#\n",
    "#   训练参数的设置\n",
    "#--------------------------------------------#\n",
    "callback        = tf.summary.create_file_writer(\"logs\")\n",
    "loss_history    = LossHistory(\"logs/\")\n",
    "\n",
    "bbox_util       = BBoxUtility(num_classes)\n",
    "roi_helper      = ProposalTargetCreator(num_classes)\n",
    "#---------------------------#\n",
    "#   读取数据集对应的txt\n",
    "#---------------------------#\n",
    "with open(train_annotation_path) as f:\n",
    "    train_lines = f.readlines()\n",
    "with open(val_annotation_path) as f:\n",
    "    val_lines   = f.readlines()\n",
    "num_train   = len(train_lines)\n",
    "num_val     = len(val_lines)\n",
    "\n",
    "freeze_layers = {'vgg' : 17, 'resnet50' : 141}[backbone]\n",
    "if Freeze_Train:\n",
    "    for i in range(freeze_layers): \n",
    "        if type(model_all.layers[i]) != tf.keras.layers.BatchNormalization:\n",
    "            model_all.layers[i].trainable = False\n",
    "    print('Freeze the first {} layers of total {} layers.'.format(freeze_layers, len(model_all.layers)))\n",
    "\n",
    "#------------------------------------------------------#\n",
    "#   主干特征提取网络特征通用，冻结训练可以加快训练速度\n",
    "#   也可以在训练初期防止权值被破坏。\n",
    "#   Init_Epoch为起始世代\n",
    "#   Freeze_Epoch为冻结训练的世代\n",
    "#   Unfreeze_Epoch总训练世代\n",
    "#   提示OOM或者显存不足请调小Batch_size\n",
    "#------------------------------------------------------#\n",
    "if True:\n",
    "    batch_size  = Freeze_batch_size\n",
    "    lr          = Freeze_lr\n",
    "    start_epoch = Init_Epoch\n",
    "    end_epoch   = Freeze_Epoch\n",
    "\n",
    "    epoch_step      = num_train // batch_size\n",
    "    epoch_step_val  = num_val // batch_size\n",
    "\n",
    "    if epoch_step == 0 or epoch_step_val == 0:\n",
    "        raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "\n",
    "    model_rpn.compile(\n",
    "        loss = {\n",
    "            'classification': rpn_cls_loss(),\n",
    "            'regression'    : rpn_smooth_l1()\n",
    "        }, optimizer = Adam(lr=lr)\n",
    "    )\n",
    "    model_all.compile(\n",
    "        loss = {\n",
    "            'classification'                        : rpn_cls_loss(),\n",
    "            'regression'                            : rpn_smooth_l1(),\n",
    "            'dense_class_{}'.format(num_classes)    : classifier_cls_loss(),\n",
    "            'dense_regress_{}'.format(num_classes)  : classifier_smooth_l1(num_classes - 1)\n",
    "        }, optimizer = Adam(lr=lr)\n",
    "    )\n",
    "\n",
    "    gen     = FRCNNDatasets(train_lines, input_shape, anchors, batch_size, num_classes, train = True).generate()\n",
    "    gen_val = FRCNNDatasets(val_lines, input_shape, anchors, batch_size, num_classes, train = False).generate()\n",
    "\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        fit_one_epoch(model_rpn, model_all, loss_history, callback, epoch, epoch_step, epoch_step_val, gen, gen_val, end_epoch,\n",
    "                anchors, bbox_util, roi_helper)\n",
    "        lr = lr*0.96\n",
    "        K.set_value(model_rpn.optimizer.lr, lr)\n",
    "        K.set_value(model_all.optimizer.lr, lr)\n",
    "\n",
    "if Freeze_Train:\n",
    "    for i in range(freeze_layers): \n",
    "        if type(model_all.layers[i]) != tf.keras.layers.BatchNormalization:\n",
    "            model_all.layers[i].trainable = True\n",
    "\n",
    "if True:\n",
    "    batch_size  = Unfreeze_batch_size\n",
    "    lr          = Unfreeze_lr\n",
    "    start_epoch = Freeze_Epoch\n",
    "    end_epoch   = UnFreeze_Epoch\n",
    "\n",
    "    epoch_step      = num_train // batch_size\n",
    "    epoch_step_val  = num_val // batch_size\n",
    "\n",
    "    if epoch_step == 0 or epoch_step_val == 0:\n",
    "        raise ValueError('数据集过小，无法进行训练，请扩充数据集。')\n",
    "\n",
    "    model_rpn.compile(\n",
    "        loss = {\n",
    "            'classification': rpn_cls_loss(),\n",
    "            'regression'    : rpn_smooth_l1()\n",
    "        }, optimizer = Adam(lr=lr)\n",
    "    )\n",
    "    model_all.compile(\n",
    "        loss = {\n",
    "            'classification'                        : rpn_cls_loss(),\n",
    "            'regression'                            : rpn_smooth_l1(),\n",
    "            'dense_class_{}'.format(num_classes)    : classifier_cls_loss(),\n",
    "            'dense_regress_{}'.format(num_classes)  : classifier_smooth_l1(num_classes - 1)\n",
    "        }, optimizer = Adam(lr=lr)\n",
    "    )\n",
    "\n",
    "    gen     = FRCNNDatasets(train_lines, input_shape, anchors, batch_size, num_classes, train = True).generate()\n",
    "    gen_val = FRCNNDatasets(val_lines, input_shape, anchors, batch_size, num_classes, train = False).generate()\n",
    "\n",
    "    print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        fit_one_epoch(model_rpn, model_all, loss_history, callback, epoch, epoch_step, epoch_step_val, gen, gen_val, end_epoch,\n",
    "                anchors, bbox_util, roi_helper)\n",
    "        lr = lr*0.96\n",
    "        K.set_value(model_rpn.optimizer.lr, lr)\n",
    "        K.set_value(model_all.optimizer.lr, lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
